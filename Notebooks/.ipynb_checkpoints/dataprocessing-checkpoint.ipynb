{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "The purpose of this notebook is to process the data and keep it separate from the main analysis and visualization. We will write the code unnecessary for the user to interact with such as functions, import libraries, and processing the data into sheets of data we can interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* content\n",
    "\n",
    "## Notebooks\n",
    "* [Overview Notebook](airpollution.ipynb)\n",
    "* [Processing Notebook](dataprocessing.ipynb)\n",
    "* [Analysis Notebook](data-analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Here are the libraries we are going to use to graph and analyze our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bd369cdf99c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from matplotlib.pyplot import figure \n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Here are the functions we have created to analyze the datasets we are looking at and graph them to display trends and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatinate_six(path):\n",
    "    files = glob(path + '*csv')\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        df['source_file'] = f.split('/')[-1]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    data = pd.concat(dfs)\n",
    "    data.columns = ['date', 'source_ozone_aqi', 'ozone_aqi',\n",
    "                    'source_pm25_aqi', 'pm25_aqi', 'file']\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data.sort_values('date').reset_index(drop=True) # sort *after* converting to datetime\n",
    "    return data\n",
    "\n",
    "def concatinate_seven(path):\n",
    "    files = glob(path + '*csv')\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        df['source_file'] = f.split('/')[-1]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    data = pd.concat(dfs)\n",
    "    data.columns = ['date', 'source_ozone_aqi', 'ozone_aqi',\n",
    "                    'source_pm25_aqi', 'pm25_aqi', 'file', '']\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data.sort_values('date').reset_index(drop=True) # sort *after* converting to datetime\n",
    "    return data\n",
    "\n",
    "def concatinate_eight(path):\n",
    "    files = glob(path + '*csv')\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        df['source_file'] = f.split('/')[-1]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    data = pd.concat(dfs)\n",
    "    data.columns = ['date', 'source_ozone_aqi', 'ozone_aqi',\n",
    "                    'source_pm25_aqi', 'pm25_aqi', 'file', '', '']\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data.sort_values('date').reset_index(drop=True) # sort *after* converting to datetime\n",
    "    return data\n",
    "\n",
    "def concatinate_nine(path):\n",
    "    files = glob(path + '*csv')\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        df['source_file'] = f.split('/')[-1]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    data = pd.concat(dfs)\n",
    "    data.columns = ['date', 'source_ozone_aqi', 'ozone_aqi',\n",
    "                    'source_pm25_aqi', 'pm25_aqi', 'file', '', '', '']\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data.sort_values('date').reset_index(drop=True) # sort *after* converting to datetime\n",
    "    return data\n",
    "\n",
    "def rolling_mean(df, series_name, z = 30):\n",
    "    s = df[['date', series_name]].dropna()\n",
    "    col = '{}_smoothed'.format(series_name)\n",
    "    s[col] = s[series_name].rolling(window=z).mean()\n",
    "    return df.merge(s[['date', col]], how='left')\n",
    "    return data\n",
    "\n",
    "def missing_time_analysis(df, series_name):\n",
    "    print('\\nAnalyzing missing time series data for \"{}\"'.format(series_name))\n",
    "    s = df[['date', series_name]].dropna()\n",
    "    s['delta_t'] = s['date'].diff()\n",
    "    s['delta_days'] = s['delta_t'].apply(lambda t: t.days)\n",
    "    display(s.head(5))\n",
    "    s.delta_days.hist(bins=50)\n",
    "    plt.title('Time Gaps (days)')\n",
    "    plt.show()\n",
    "    display(s['delta_days'].describe())\n",
    "    display(s[s.delta_days > 5]) # show any gaps larger than 5 days\n",
    "    display(s[s['delta_days'] > 5].groupby('delta_days').size())\n",
    "        \n",
    "\n",
    "def aqi_plot(x, y): \n",
    "    \n",
    "    ax = x.plot.line(\n",
    "    'date', ['ozone_aqi_smoothed', 'pm25_aqi_smoothed'],\n",
    "    figsize=(16, 10), colormap='Paired',\n",
    "    label=['Ozone', 'PM 2.5'])\n",
    "    \n",
    "    ax.set_title(y + ' Air Quality Trend', fontsize=18)\n",
    "    ax.set_xlabel('Time (1985-2020)', fontsize=14)\n",
    "    ax.set_ylabel('Average AQI', fontsize=14)\n",
    "    ax.legend(prop={'size': 14})\n",
    "    ax.grid()\n",
    "\n",
    "def year_plot(x, y):\n",
    "    data = x\n",
    "    data['ozone_aqi'] = x['Ozone_AQI_Value'].rolling(window=30).mean()\n",
    "    data['pm25_aqi'] = x['PM2.5_AQI_Value'].rolling(window=30).mean()\n",
    "    \n",
    "    ax = data.plot.line('Date', ['ozone_aqi', 'pm25_aqi'], figsize = (16, 10), colormap = 'Paired',\n",
    "                     label = ['Ozone', 'PM 2.5'])\n",
    "    ax.set_title(y + 'Air Quality Trend', fontsize = 18)\n",
    "    ax.set_xlabel('Time (2020)', fontsize = 14)\n",
    "    ax.set_ylabel('AQI', fontsize = 14)\n",
    "    ax.legend(prop = {'size' : 14})\n",
    "    ax.grid()\n",
    "    \n",
    "def plot_all(x):\n",
    "    \n",
    "\n",
    "    data = x[0]\n",
    "    \n",
    "   \n",
    "    for f in range (1, len(x)):\n",
    "        data = pd.merge(data, x[f], how = 'inner', on = 'date')\n",
    "    ax = data.plot.line('date', ['ozone_aqi_smoothed_x', \n",
    "                                 'ozone_aqi_smoothed_y', 'ozone_aqi_smoothed'], figsize = (20, 10), title = 'Ozone Quality Trend', colormap = 'Paired')\n",
    "    ax.set_xlabel('Time (1985-2020)')\n",
    "    ax.set_ylabel('Average AQI')\n",
    "    ax.legend(['NY-NJ-PA', 'TX', 'MA-NH', 'CA', 'PA-NJ-DE-MD', 'WA', 'MN-WI', 'IL-IN-WI', 'GA', \n",
    "              'MI', 'FL', 'AZ', 'DC-VA-MD-WV', 'CO', 'MO-IL'])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    bx = data.plot.line('date', ['pm25_aqi_smoothed_x',\n",
    "                                 'pm25_aqi_smoothed_y', 'pm25_aqi_smoothed'], figsize = (20, 10), title = 'Particulate Matter Quality Trend', colormap = 'Paired')\n",
    "    bx.set_xlabel('Time (1985-2020)')\n",
    "    bx.set_ylabel('Average AQI')\n",
    "    bx.legend(['NY-NJ-PA', 'TX', 'MA-NH', 'CA', 'PA-NJ-DE-MD', 'WA', 'MN-WI', 'IL-IN-WI', 'GA', \n",
    "              'MI', 'FL', 'AZ', 'DC-VA-MD-WV', 'CO', 'MO-IL'])\n",
    "    bx.grid(True)\n",
    "\n",
    "def production(x, z):\n",
    "\n",
    "    y = x\n",
    "    goods = ['Gasoline', 'Fuel oils', 'Coal n.e.c', 'Motorized vehicles', 'Transport equipment', 'Machinery'\n",
    "                   , 'Chemical products', 'Newsprint/paper', 'Printed products']\n",
    "\n",
    "    y = y.sort_values(by = ['ID SCTG2', 'Year'], ascending = True)\n",
    "\n",
    "    gasoline_index = [y.loc[y['SCTG2'] == 'Gasoline'].index.values.tolist()]\n",
    "    gasoline = []\n",
    "    for i in range (5):\n",
    "        gasoline.append(x.iat[gasoline_index[0][i], 4])\n",
    "    \n",
    "    fuel_oils_index = [y.loc[y['SCTG2'] == 'Fuel oils'].index.values.tolist()]\n",
    "    fuel_oils = []\n",
    "    for i in range (5):\n",
    "        fuel_oils.append(x.iat[fuel_oils_index[0][i], 4])\n",
    "        \n",
    "    coal_nec_index = [y.loc[y['SCTG2'] == 'Coal-n.e.c.'].index.values.tolist()]\n",
    "    coal_nec = []\n",
    "    for i in range (5):\n",
    "        coal_nec.append(x.iat[coal_nec_index[0][i], 4])\n",
    "\n",
    "    motorized_vehicles_index = [y.loc[y['SCTG2'] == 'Motorized vehicles'].index.values.tolist()]\n",
    "    motorized_vehicles = []\n",
    "    for i in range (5):\n",
    "        motorized_vehicles.append(x.iat[motorized_vehicles_index[0][i], 4])\n",
    "    \n",
    "    transport_equipment_index = [y.loc[y['SCTG2'] == 'Transport equip.'].index.values.tolist()]\n",
    "    transport_equipment = []\n",
    "    for i in range (5):\n",
    "        transport_equipment.append(x.iat[transport_equipment_index[0][i], 4])\n",
    "    \n",
    "    machinery_index = [y.loc[y['SCTG2'] == 'Machinery'].index.values.tolist()]\n",
    "    machinery = []\n",
    "    for i in range (5):\n",
    "        machinery.append(x.iat[machinery_index[0][i], 4])\n",
    "        \n",
    "    \n",
    "    newsprint_paper_index = [y.loc[y['SCTG2'] == 'Newsprint/paper'].index.values.tolist()]\n",
    "    newsprint_paper = []\n",
    "    for i in range (5):\n",
    "        newsprint_paper.append(x.iat[newsprint_paper_index[0][i], 4])\n",
    "    \n",
    "    printed_products_index = [y.loc[y['SCTG2'] == 'Printed prods.'].index.values.tolist()]\n",
    "    printed_products = []\n",
    "    for i in range (5):\n",
    "        printed_products.append(x.iat[printed_products_index[0][i], 4])\n",
    "        \n",
    "        \n",
    "    time = ['2012', '2013', '2014', '2015', '2020']\n",
    "    \n",
    "    df = pd.DataFrame({'Year' : time,\n",
    "                       'Gasoline' : gasoline, \n",
    "                       'Fuel oils' : fuel_oils,\n",
    "                       'Coal n.e.c.' : coal_nec,\n",
    "                       'Motorized vehicles' : motorized_vehicles,\n",
    "                       'Transport equipment' : transport_equipment,\n",
    "                       'Machinery' : machinery,\n",
    "                       'Newsprint paper' : newsprint_paper,\n",
    "                       'Printed products' : printed_products})\n",
    "\n",
    "    bx = df.plot.bar('Year', ['Gasoline', 'Fuel oils', 'Coal n.e.c.', 'Motorized vehicles', 'Transport equipment',\n",
    "                          'Machinery', 'Newsprint paper', 'Printed products'], figsize = (15, 10), title = z, ylim = (0, 300000))\n",
    "    bx.set_xlabel('Year')\n",
    "    bx.set_ylabel('Millions of Dollars')\n",
    "    bx.grid(True)\n",
    "    bx.legend(['Gasoline', 'Fuel oils', 'Coal n.e.c', 'Motorized vehicles', 'Transport equipment', 'Machinery',\n",
    "                'Newsprint/paper', 'Printed products'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing The Data\n",
    "\n",
    "Here we access the file locations for each metropolitan statistical area and use the methods to create dataframes we can work with so we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_nj_pa_files = concatinate_six('/Users/smaslam/Desktop/airpollution/airpollution/Areas/NewYork-Newark-JerseyCity/')\n",
    "ny_nj_pa = rolling_mean(ny_nj_pa_files, 'ozone_aqi')\n",
    "ny_nj_pa = rolling_mean(ny_nj_pa, 'pm25_aqi')\n",
    "ny_nj_pa_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/NY-NJ-PA-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "ny_nj_pa_2020 = pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Areas/NewYork-Newark-JerseyCity/2020-NY-NJ-PA.csv')\n",
    "\n",
    "\n",
    "il_in_wi_files = concatinate_seven('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Chicago-Naperville-Elgin/')\n",
    "il_in_wi = rolling_mean(il_in_wi_files, 'ozone_aqi')\n",
    "il_in_wi = rolling_mean(il_in_wi, 'pm25_aqi')\n",
    "il_in_wi_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/IL-IN-WI-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "il_in_wi_2020 = pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Denver-Aurora-Lakewood/2020-CO.csv')\n",
    "\n",
    "tx_files = concatinate_six('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Dallas-FortWorth-Arlington/')\n",
    "tx = rolling_mean(tx_files, 'ozone_aqi')\n",
    "tx = rolling_mean(tx, 'pm25_aqi')\n",
    "tx_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/TX-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "ga_files = concatinate_six('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Atlanta-SandySprings-Roswell/')\n",
    "ga = rolling_mean(ga_files, 'ozone_aqi')\n",
    "ga = rolling_mean(ga, 'pm25_aqi')\n",
    "ga_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/GA-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "ma_nh_files = concatinate_seven('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Boston-Cambridge-Newton/')\n",
    "ma_nh = rolling_mean(ma_nh_files, 'ozone_aqi')\n",
    "ma_nh = rolling_mean(ma_nh, 'pm25_aqi')\n",
    "ma_nh_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/MA-NH-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "mi_files = concatinate_six('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Detroit-Warren-Dearborn/')\n",
    "mi = rolling_mean(mi_files, 'ozone_aqi')\n",
    "mi = rolling_mean(mi, 'pm25_aqi')\n",
    "mi_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/MI-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "mi_2020 = pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Detroit-Warren-Dearborn/2020-MI.csv')\n",
    "\n",
    "ca_files = concatinate_seven('/Users/smaslam/Desktop/airpollution/airpollution/Areas/LosAngeles-LongBeach-Anaheim/')\n",
    "ca = rolling_mean(ca_files, 'ozone_aqi')\n",
    "ca = rolling_mean(ca, 'pm25_aqi')\n",
    "ca_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/CA-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "fl_files = concatinate_eight('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Miami-FortLauderdale-WestPalmBeach/')\n",
    "fl = rolling_mean(fl_files, 'ozone_aqi')\n",
    "fl = rolling_mean(fl, 'pm25_aqi')\n",
    "fl_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/FL-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "pa_nj_de_md_files = concatinate_six('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Philadelphia-Camden-Wilmington/')\n",
    "pa_nj_de_md = rolling_mean(pa_nj_de_md_files, 'ozone_aqi')\n",
    "pa_nj_de_md = rolling_mean(pa_nj_de_md, 'pm25_aqi')\n",
    "pa_nj_de_md_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/PA-NJ-DE-MD-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "az_files = concatinate_eight('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Phoenix-Mesa-Scottsdale/')\n",
    "az = rolling_mean(az_files, 'ozone_aqi')\n",
    "az = rolling_mean(az, 'pm25_aqi')\n",
    "az_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/AZ-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "az_2020 = pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Phoenix-Mesa-Scottsdale/2020-AZ.csv')\n",
    "\n",
    "wa_files = concatinate_six('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Seattle-Tacoma-Bellevue/')\n",
    "wa = rolling_mean(wa_files, 'ozone_aqi')\n",
    "wa = rolling_mean(wa, 'pm25_aqi')\n",
    "wa_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/WA-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "dc_va_md_wv_files = concatinate_six('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Washington-Arlington-Alexandria/')\n",
    "dc_va_md_wv = rolling_mean(dc_va_md_wv_files, 'ozone_aqi')\n",
    "dc_va_md_wv = rolling_mean(dc_va_md_wv, 'pm25_aqi')\n",
    "dc_va_md_wv_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/DC-VA-MD-WV-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "mn_wi_files = concatinate_seven('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Minneapolis-St.Paul-Bloomington/')\n",
    "mn_wi = rolling_mean(mn_wi_files, 'ozone_aqi')\n",
    "mn_wi = rolling_mean(mn_wi, 'pm25_aqi')\n",
    "mn_wi_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/MN-WI-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "co_files = concatinate_seven('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Denver-Aurora-Lakewood/')\n",
    "co = rolling_mean(co_files, 'ozone_aqi')\n",
    "co = rolling_mean(co, 'pm25_aqi')\n",
    "co_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/CO-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "co_2020 = pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Areas/Denver-Aurora-Lakewood/2020-CO.csv')\n",
    "\n",
    "mo_il_files = concatinate_seven('/Users/smaslam/Desktop/airpollution/airpollution/Areas/St.Louis/')\n",
    "mo_il = rolling_mean(mo_il_files, '')\n",
    "mo_il['ozone_aqi_smoothed'] = mo_il['_smoothed']\n",
    "mo_il = rolling_mean(mo_il, 'pm25_aqi')\n",
    "mo_il_production = pd.DataFrame(pd.read_csv('/Users/smaslam/Desktop/airpollution/airpollution/Production/MO-IL-Production.csv',\n",
    "                                         sep = r'\\s*,\\s*', engine = 'python'))\n",
    "\n",
    "areas = [ny_nj_pa, il_in_wi, tx, ga, ma_nh, mi, ca, fl, pa_nj_de_md, az, wa, dc_va_md_wv, mn_wi, co, mo_il]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
